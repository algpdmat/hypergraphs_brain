{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_dIasUcmEk4z"
      },
      "outputs": [],
     "source": [
    "from networkx.readwrite import json_graph  # Ferramenta para ler/escrever grafos em formato JSON",
    "from scipy.stats import percentileofscore   # Função para calcular o percentil de uma pontuação em uma lista",
    "from traceback import format_exception      # Função para formatar a saída de exceções",
    "import cProfile                             # Módulo para análise de desempenho de código Python",
    "import json                                 # Módulo para trabalhar com dados em formato JSON",
    "import networkx as nx                       # Biblioteca para criação, manipulação e estudo de grafos e redes complexas",
    "import numpy as np                          # Biblioteca para operações matemáticas e manipulação de arrays",
    "import pandas as pd                         # Biblioteca para manipulação e análise de dados",
    "import pstats                               # Utilitários para imprimir estatísticas de análise de desempenho",
    "import random                               # Módulo para geração de números aleatórios",
    "import sys                                  # Módulo para acessar funções e variáveis específicas do Python",
    "import glob                                 # Função para encontrar todos os nomes de caminhos que correspondem a um padrão especificado",
    "import os                                   # Módulo para interagir com o sistema operacional"
]

    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRAINDO AS MATRIZES DE CONECTIVIDADE DTI DO DRIVE\n",
        "files_autism=glob.glob('/content/drive/MyDrive/metricas/UCLA_Autism/*ASD*DTI*connectivity_matrix*')\n",
        "files_control=glob.glob('/content/drive/MyDrive/metricas/UCLA_Autism/*TD*DTI*connectivity_matrix*')\n",
        "\n",
        "\n",
        "# DROPANDO OS PRIMEIROS ELEMENTOS DA LISTA POIS SAO MATRIZES ALL\n",
        "files_autism.pop(0)\n",
        "files_control.pop(0)"
      ],
      "metadata": {
        "id": "6WC5eD1YE3L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRECISAMOS NORMALIZAR E SIMETRIZARAS MATRIZES\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mat_aut=[]\n",
        "for i in files_autism:  # MATRIZES ASD (AUT)\n",
        "    df = pd.read_csv(i ,sep=\"  \", header=None).astype(float) \n",
        "    scaler = MinMaxScaler()\n",
        "    sub_norm=pd.DataFrame(scaler.fit_transform(df.T).T,columns=df.columns)\n",
        "    sub_norm = sub_norm.to_numpy()\n",
        "    sub = (sub_norm + sub_norm.T)/2\n",
        "    #matrix=sub.to_numpy() \n",
        "    mat_aut.append(sub)\n",
        "\n",
        "mat_control=[]\n",
        "for i in files_control: # MATRIZES TD (CONTROL)\n",
        "    df = pd.read_csv(i ,sep=\"  \", header=None).astype(float) \n",
        "    scaler = MinMaxScaler()\n",
        "    sub_norm=pd.DataFrame(scaler.fit_transform(df.T).T,columns=df.columns)\n",
        "    sub_norm = sub_norm.to_numpy()\n",
        "    sub = (sub_norm + sub_norm.T)/2\n",
        "    #matrix=sub.to_numpy() \n",
        "    mat_control.append(sub)"
      ],
      "metadata": {
        "id": "npuXarhYFJDM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCAOUSADA PARA GERAR GRAFOS COM DENSIDADE\n",
        "def G_den(matrix, d, verbose=False):\n",
        "    \"\"\"Returns a networkx Graph from a adjacency matrix, with a given density d\n",
        "        \n",
        "    Parameters\n",
        "    ----------\n",
        "    matrix: matrix\n",
        "        A matrix of values - connectivity matrix\n",
        "    \n",
        "    d: float\n",
        "        Density value for matrix binaziring \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "        NetworkX graph\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    #matrix i, density d. i is a matrix - ravel flatten the matrix\n",
        "    np.fill_diagonal(matrix,0)\n",
        "    temp = sorted(matrix.ravel(), reverse=True) # will flatten it and rank corr values\n",
        "    size = len(matrix)\n",
        "    cutoff = np.ceil(d * (size * (size-1))) # number of links with a given density\n",
        "    tre = temp[int(cutoff)]\n",
        "    G0 = nx.from_numpy_array(matrix)\n",
        "    G0.remove_edges_from(list(nx.selfloop_edges(G0)))\n",
        "    G1 = nx.from_numpy_array(matrix)\n",
        "    for u,v,a in G0.edges(data=True):\n",
        "        if (a.get('weight')) <= tre:\n",
        "            G1.remove_edge(u, v)\n",
        "    finaldensity = nx.density(G1)\n",
        "    if verbose == True:\n",
        "        print(finaldensity)\n",
        "        \n",
        "    return G1"
      ],
      "metadata": {
        "id": "4pl0uLbLFQMD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AFIM DE FILTAR OS GRAFOS USANDO DISPARITY FILTER NOS VAMOS CRIAR GRAFOS CHEIOS PARA CADA GRUPO DE MATRIZES (AUT E CONTROL)\n",
        "\n",
        "grafos_aut = []\n",
        "for i in mat_aut:\n",
        "  a = G_den(i, 1)\n",
        "  grafos_aut.append(a)\n",
        "\n",
        "grafos_control = []\n",
        "for i in mat_control:\n",
        "  a = G_den(i, 1)\n",
        "  grafos_control.append(a)"
      ],
      "metadata": {
        "id": "Gc8hFqkmFePM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AQUI VAMOS FAZER OS CORTES BASEADOS NUM VALOR DE DISPARITY FILTER 0.60, O VALOR FOI USADO AFIM DE SE OBTER UM NUMERO DE TRIANGULOS SIGNIFICANTES PARA ANALISE ESTATISTICA\n",
        "\n",
        "def apply_disparity_filter(graph):\n",
        "    \"\"\"\n",
        "    Aplica o filtro de disparidade ao grafo e retorna um novo grafo apenas com as arestas filtradas.\n",
        "    \"\"\"\n",
        "    filtered_graph = nx.Graph()\n",
        "\n",
        "    # Calcula os valores de disparidade para cada aresta do grafo\n",
        "    alpha_measures = disparity_filter(graph)\n",
        "\n",
        "    # Itera sobre as arestas do grafo e verifica o valor de disparidade\n",
        "    for u, v, a in graph.edges(data=True):\n",
        "        alpha = a.get('alpha', 0.0)  # Obtém o valor de disparidade da aresta, considerando 0.0 caso não exista\n",
        "        if alpha in alpha_measures and alpha >= 0.60:\n",
        "            filtered_graph.add_edge(u, v, alpha=alpha)\n",
        "\n",
        "    return filtered_graph\n",
        "\n",
        "\n",
        "# AQUI ESTA O LOOPING CORTANDO OS GRAFOS E SALVANDO PARA ANALISES \n",
        "\n",
        "\n",
        "# Diretório de destino para salvar os grafos filtrados\n",
        "output_dir = \"/content/drive/MyDrive/metricas/graficos filtrados/grafos cortados\"\n",
        "\n",
        "# Verifica se o diretório de destino existe, caso contrário, cria-o\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Itera sobre os grafos de entrada\n",
        "for i, gf in enumerate(grafos_aut):\n",
        "    # Aplica o filtro de disparidade ao grafo\n",
        "    filtered_graph = apply_disparity_filter(gf)\n",
        "\n",
        "    # Define o caminho do arquivo de saída\n",
        "    output_path = os.path.join(output_dir, f\"grafo_cortado_{i}.txt\")\n",
        "\n",
        "    # Salva o grafo filtrado como arquivo de texto\n",
        "    nx.write_adjlist(filtered_graph, output_path)\n",
        "\n",
        "    print(f\"Grafo {i} salvo em: {output_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# Diretório de destino para salvar os grafos filtrados\n",
        "output_dir = \"/content/drive/MyDrive/metricas/graficos filtrados/grafos cortados 2\"\n",
        "\n",
        "# Verifica se o diretório de destino existe, caso contrário, cria-o\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Itera sobre os grafos de entrada\n",
        "for i, gf in enumerate(grafos_control):\n",
        "    # Aplica o filtro de disparidade ao grafo\n",
        "    filtered_graph = apply_disparity_filter(gf)\n",
        "\n",
        "    # Define o caminho do arquivo de saída\n",
        "    output_path = os.path.join(output_dir, f\"grafo_cortado_control_{i}.txt\")\n",
        "\n",
        "    # Salva o grafo filtrado como arquivo de texto\n",
        "    nx.write_adjlist(filtered_graph, output_path)\n",
        "\n",
        "    print(f\"Grafo {i} salvo em: {output_path}\")\n"
      ],
      "metadata": {
        "id": "R70OtONjF4Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AGORA VAMOS IMPORTAR OS GRAFOS CORTADOS "
      ],
      "metadata": {
        "id": "RgX7shwJGhGq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extrair_grafos(diretorio):\n",
        "    grafos = []\n",
        "    for nome_arquivo in os.listdir(diretorio):\n",
        "        if nome_arquivo.endswith('.txt') and nome_arquivo.startswith('grafo_cortado_'):\n",
        "            caminho_arquivo = os.path.join(diretorio, nome_arquivo)\n",
        "            grafo = nx.read_adjlist(caminho_arquivo)\n",
        "            grafos.append(grafo)\n",
        "    return grafos\n",
        "\n",
        "# Extrair grafos da pasta de controle\n",
        "diretorio_aut = '/content/drive/MyDrive/metricas/graficos filtrados/grafos cortados/'\n",
        "aut_graphs = extrair_grafos(diretorio_aut)\n",
        "\n",
        "diretorio_control = '/content/drive/MyDrive/metricas/graficos filtrados/grafos cortados 2/'\n",
        "control_graphs = extrair_grafos(diretorio_control)\n"
      ],
      "metadata": {
        "id": "5jhFLZt-Go5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "\n",
        "def get_top_nodes(graphs):\n",
        "  # Inputs:\n",
        "# - graphs: uma lista de grafos representando redes complexas\n",
        "\n",
        "# Outputs:\n",
        "# - top_nodes_bc_df: DataFrame com os nós mais importantes e suas estatísticas de Betweenness Centrality\n",
        "# - top_nodes_dc_df: DataFrame com os nós mais importantes e suas estatísticas de Degree Centrality\n",
        "# - top_nodes_ec_df: DataFrame com os nós mais importantes e suas estatísticas de Eigenvector Centrality\n",
        "\n",
        "# Funcionalidade:\n",
        "# A função recebe uma lista de grafos e calcula as métricas de centralidade (Betweenness Centrality, Degree Centrality e Eigenvector Centrality) para cada grafo.\n",
        "# Em seguida, identifica os 10 nós mais importantes para cada métrica em cada grafo e armazena suas informações em DataFrames separados.\n",
        "# Os DataFrames finais contêm os nós mais importantes e suas estatísticas de centralidade para cada métrica.\n",
        "# A função ajuda a identificar os nós mais relevantes em uma rede complexa, fornecendo insights sobre sua estrutura e importância relativa.\n",
        "\n",
        "    # Listas para armazenar os nós mais importantes para cada métrica\n",
        "    top_nodes_bc = []\n",
        "    top_nodes_dc = []\n",
        "    top_nodes_ec = []\n",
        "\n",
        "    for graph in graphs:\n",
        "        # Cálculo das métricas de centralidade\n",
        "        bc = nx.betweenness_centrality(graph)  # Betweenness Centrality\n",
        "        dc = nx.degree_centrality(graph)  # Degree Centrality\n",
        "        ec = nx.eigenvector_centrality(graph)  # Eigenvector Centrality\n",
        "\n",
        "        # Ordenação dos nós em cada métrica\n",
        "        sorted_bc = sorted(bc, key=bc.get, reverse=True)\n",
        "        sorted_dc = sorted(dc, key=dc.get, reverse=True)\n",
        "        sorted_ec = sorted(ec, key=ec.get, reverse=True)\n",
        "\n",
        "        # Adição dos nós mais importantes e seus valores à lista\n",
        "        top_nodes_bc.extend({'nodes': node, 'BC': bc[node]} for node in sorted_bc[:10])\n",
        "        top_nodes_dc.extend({'nodes': node, 'DC': dc[node]} for node in sorted_dc[:10])\n",
        "        top_nodes_ec.extend({'nodes': node, 'EC': ec[node]} for node in sorted_ec[:10])\n",
        "\n",
        "    # Criação dos DataFrames para cada métrica de centralidade\n",
        "    top_nodes_bc_df = pd.DataFrame(top_nodes_bc)\n",
        "    top_nodes_dc_df = pd.DataFrame(top_nodes_dc)\n",
        "    top_nodes_ec_df = pd.DataFrame(top_nodes_ec)\n",
        "\n",
        "    # Contagem e média dos valores de BC por nó\n",
        "    count_bc = top_nodes_bc_df['nodes'].value_counts().reset_index()\n",
        "    count_bc.columns = ['nodes', 'count']\n",
        "    mean_bc = top_nodes_bc_df.groupby('nodes')['BC'].mean().reset_index()\n",
        "    top_nodes_bc_df = pd.merge(count_bc, mean_bc, on='nodes')\n",
        "\n",
        "    # Contagem e média dos valores de DC por nó\n",
        "    count_dc = top_nodes_dc_df['nodes'].value_counts().reset_index()\n",
        "    count_dc.columns = ['nodes', 'count']\n",
        "    mean_dc = top_nodes_dc_df.groupby('nodes')['DC'].mean().reset_index()\n",
        "    top_nodes_dc_df = pd.merge(count_dc, mean_dc, on='nodes')\n",
        "\n",
        "    # Contagem e média dos valores de EC por nó\n",
        "    count_ec = top_nodes_ec_df['nodes'].value_counts().reset_index()\n",
        "    count_ec.columns = ['nodes', 'count']\n",
        "    mean_ec = top_nodes_ec_df.groupby('nodes')['EC'].mean().reset_index()\n",
        "    top_nodes_ec_df = pd.merge(count_ec, mean_ec, on='nodes')\n",
        "\n",
        "    return top_nodes_bc_df, top_nodes_dc_df, top_nodes_ec_df\n"
      ],
      "metadata": {
        "id": "NJq-TUamHXEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOMANDO OS HUBS DOS GRAFOS CORTADOS\n",
        "hubs_g_aut = get_top_nodes(aut_graphs)\n",
        "hubs_g_control = get_top_nodes(control_graphs)\n",
        "\n",
        "#HUBS PARA O GRUPO CONTROL\n",
        "hubs_g_control_bc = hubs_g_control[0]\n",
        "hubs_g_control_dc = hubs_g_control[1]\n",
        "hubs_g_control_ec = hubs_g_control[2]\n",
        "\n",
        "#HUBS PARA O GRUPO AUT\n",
        "hubs_g_aut_bc = hubs_g_aut[0]\n",
        "hubs_g_aut_dc = hubs_g_aut[1]\n",
        "hubs_g_aut_ec = hubs_g_aut[2]"
      ],
      "metadata": {
        "id": "Zhjf1jDoH1sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CRIANDO OS HYPERGRAFOS PARA CALCULAR OS HUBS\n",
        "\n",
        "# O RETURN DA FUNCAO SAO AS LISTAS DE CLIQUES POIS ESTAMOS INTERESSADO NOS HUBS\n",
        "\n",
        "def HG_aut(graphs):\n",
        "    HG_aut = []\n",
        "    tri_aut = []\n",
        "    for i, G in enumerate(graphs):\n",
        "        all_cliques = nx.enumerate_all_cliques(G)\n",
        "        triangles = [x for x in all_cliques if len(x) == 3]\n",
        "        tri_aut.append(triangles)\n",
        "        size = len(triangles)\n",
        "        Hypergraph = np.empty((size, size))\n",
        "        for i in range(0, len(triangles)):\n",
        "            for j in range(0, len(triangles)):\n",
        "                a = set(triangles[i])\n",
        "                b = set(triangles[j])\n",
        "                if len(a & b) == 2:\n",
        "                    Hypergraph[i, j] = int(1)\n",
        "        HG_aut.append(Hypergraph.astype(int))\n",
        "        # Save each triangle set to a separate file\n",
        "        for j, triangle_set in enumerate(triangles):\n",
        "            triangle_set = np.array(triangle_set, dtype=int)\n",
        "\n",
        "    return tri_aut\n",
        "\n",
        "def HG_control(graphs):\n",
        "    HG_control = []\n",
        "    tri_control = []\n",
        "    for i, G in enumerate(graphs):\n",
        "        all_cliques = nx.enumerate_all_cliques(G)\n",
        "        triangles = [x for x in all_cliques if len(x) == 3]\n",
        "        tri_control.append(triangles)\n",
        "        size = len(triangles)\n",
        "        Hypergraph = np.empty((size, size))\n",
        "        for i in range(0, len(triangles)):\n",
        "            for j in range(0, len(triangles)):\n",
        "                a = set(triangles[i])\n",
        "                b = set(triangles[j])\n",
        "                if len(a & b) == 2:\n",
        "                    Hypergraph[i, j] = int(1)\n",
        "        HG_control.append(Hypergraph.astype(int))\n",
        "        # Save each triangle set to a separate file\n",
        "        for j, triangle_set in enumerate(triangles):\n",
        "            triangle_set = np.array(triangle_set, dtype=int)\n",
        "\n",
        "    return tri_control"
      ],
      "metadata": {
        "id": "QMkd7DcoIt-J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLIQUES PARA OS GRUPOS CONTROL E AUT\n",
        "\n",
        "\n",
        "tri_control = HG_control(control_graphs) \n",
        "tri_aut = HG_aut(aut_graphs)"
      ],
      "metadata": {
        "id": "Im_Yiv2fI1XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCULANDO OS HUBS DOS GRAFOS E HYPERGRAFOS\n",
        "\n",
        "hubs_aut_hp = []\n",
        "for cliques in tri_aut:\n",
        "    c = calculate_dual_centralities(cliques)\n",
        "    hubs_aut_hp.append(c)\n",
        "\n",
        "hubs_control_hp = []\n",
        "for cliques in tri_control:\n",
        "    c = calculate_dual_centralities(cliques)\n",
        "    hubs_control_hp.append(c)"
      ],
      "metadata": {
        "id": "y5CDVMYmM1y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SEPARANDO OS DATAFRAMES POR METRICAS\n",
        "\n",
        "def split_dataframe(df, metric):\n",
        "    # Resetar o índice para tornar a coluna 'Clique' em uma coluna regular\n",
        "    df = df.reset_index()\n",
        "\n",
        "    # Ordenar o DataFrame pela métrica desejada em ordem decrescente\n",
        "    sorted_df = df.sort_values(by=metric, ascending=False)\n",
        "\n",
        "    # Criar o DataFrame separado contendo as colunas 'Clique' e a métrica\n",
        "    result_df = pd.DataFrame({'Clique': sorted_df['Clique'], metric: sorted_df[metric]})\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "hubs_aut_hp_split = []\n",
        "for df in hubs_aut_hp:\n",
        "    df_bc_aut = split_dataframe(df, 'BC')\n",
        "    df_dc_aut = split_dataframe(df, 'DC')\n",
        "    df_ec_aut = split_dataframe(df, 'EC')\n",
        "\n",
        "    # Adicionar os DataFrames separados à lista hubs_aut_hp_split\n",
        "    hubs_aut_hp_split.append(df_bc_aut)\n",
        "    hubs_aut_hp_split.append(df_dc_aut)\n",
        "    hubs_aut_hp_split.append(df_ec_aut)\n",
        "\n",
        "\n",
        "hubs_control_hp_split = []\n",
        "for df in hubs_control_hp:\n",
        "    df_bc_control = split_dataframe(df, 'BC')\n",
        "    df_dc_control = split_dataframe(df, 'DC')\n",
        "    df_ec_control = split_dataframe(df, 'EC')\n",
        "\n",
        "    # Adicionar os DataFrames separados à lista hubs_aut_hp_split\n",
        "    hubs_control_hp_split.append(df_bc_control)\n",
        "    hubs_control_hp_split.append(df_dc_control)\n",
        "    hubs_control_hp_split.append(df_ec_control)"
      ],
      "metadata": {
        "id": "yBcvx-P7NMun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ASSOCIANDO OS CLIQUES AS REGIOES DA LISTA NOMES\n",
        "\n",
        "def associar_cliques_regioes(df, lista_nomes, metrica):\n",
        "    cliques_regioes = []\n",
        "    for clique in df['Clique']:\n",
        "        regioes_clique = [lista_nomes[int(indice)-1] for indice in clique]\n",
        "        cliques_regioes.append(regioes_clique)\n",
        "\n",
        "    df_resultado = pd.DataFrame({'Clique': df['Clique'], metrica: df[metrica], 'Regioes': cliques_regioes})\n",
        "    return df_resultado\n",
        "\n",
        "\n",
        "# HUBS COM NOMES DAS REGIOES DOS HYPERGRAFOS\n",
        "\n",
        "df_bc_aut_nomes = associar_cliques_regioes(df_bc_aut, nomes, 'BC')\n",
        "df_dc_aut_nomes = associar_cliques_regioes(df_dc_aut, nomes, 'DC')\n",
        "df_ec_aut_nomes = associar_cliques_regioes(df_ec_aut, nomes, 'EC')\n",
        "\n",
        "df_bc_control_nomes = associar_cliques_regioes(df_bc_control, nomes, 'BC')\n",
        "df_dc_control_nomes = associar_cliques_regioes(df_dc_control, nomes, 'DC')\n",
        "df_ec_control_nomes = associar_cliques_regioes(df_ec_control, nomes, 'EC')"
      ],
      "metadata": {
        "id": "hdLZ3hjlNW9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### FAZENDO A MESMA COISA AGORA PARA OS GRAFOS E HYPERGRAFOS COM DENSIDADE 5% ########\n",
        "### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ ##############\n",
        "# A PRIMEIRA ETAPA DE EXTRAIR AS MATRIZES E SIMETRIZAR É A MESMA PARA AMBOS OS METODOS"
      ],
      "metadata": {
        "id": "xufgxooTNjki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USAMOS A MESMA FUNCAO PARA CRIAR OS GRAFOS COM DENSIDADE 5% PARA AMBOS OS GRUPOS\n",
        "\n",
        "grafos_aut = []\n",
        "for i in mat_aut:\n",
        "  a = G_den(i, 0.05)\n",
        "  grafos_aut.append(a)\n",
        "\n",
        "grafos_controll = []\n",
        "for i in mat_control:\n",
        "  a = G_den(i, 0.05)\n",
        "  grafos_controll.append(a)"
      ],
      "metadata": {
        "id": "WzwiGYz8Njwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCO QUE CALCULA AS CENTRALIDADES É A MESMA\n",
        "\n",
        "def get_top_nodes(graphs):\n",
        "    top_nodes_bc = []\n",
        "    top_nodes_dc = []\n",
        "    top_nodes_ec = []\n",
        "\n",
        "    for graph in graphs:\n",
        "        # Cálculo das métricas de centralidade\n",
        "        bc = nx.betweenness_centrality(graph)\n",
        "        dc = nx.degree_centrality(graph)\n",
        "        ec = nx.eigenvector_centrality(graph)\n",
        "\n",
        "        # Ordenação dos nós em cada métrica\n",
        "        sorted_bc = sorted(bc, key=bc.get, reverse=True)\n",
        "        sorted_dc = sorted(dc, key=dc.get, reverse=True)\n",
        "        sorted_ec = sorted(ec, key=ec.get, reverse=True)\n",
        "  \n",
        "        # Adição dos nós mais importantes e seus valores à lista\n",
        "        top_nodes_bc.extend({'nodes': node, 'BC': bc[node]} for node in sorted_bc[:10])\n",
        "        top_nodes_dc.extend({'nodes': node, 'DC': dc[node]} for node in sorted_dc[:10])\n",
        "        top_nodes_ec.extend({'nodes': node, 'EC': ec[node]} for node in sorted_ec[:10])\n",
        "\n",
        "    top_nodes_bc_df = pd.DataFrame(top_nodes_bc)\n",
        "    top_nodes_dc_df = pd.DataFrame(top_nodes_dc)\n",
        "    top_nodes_ec_df = pd.DataFrame(top_nodes_ec)\n",
        "\n",
        "    # Contagem e média dos valores de BC por nó\n",
        "    count_bc = top_nodes_bc_df['nodes'].value_counts().reset_index()\n",
        "    count_bc.columns = ['nodes', 'count']\n",
        "    mean_bc = top_nodes_bc_df.groupby('nodes')['BC'].mean().reset_index()\n",
        "    top_nodes_bc_df = pd.merge(count_bc, mean_bc, on='nodes')\n",
        "\n",
        "    count_dc = top_nodes_dc_df['nodes'].value_counts().reset_index()\n",
        "    count_dc.columns = ['nodes', 'count']\n",
        "    mean_dc = top_nodes_dc_df.groupby('nodes')['DC'].mean().reset_index()\n",
        "    top_nodes_dc_df = pd.merge(count_dc, mean_dc, on='nodes')\n",
        "\n",
        "    count_ec = top_nodes_ec_df['nodes'].value_counts().reset_index()\n",
        "    count_ec.columns = ['nodes', 'count']\n",
        "    mean_ec = top_nodes_ec_df.groupby('nodes')['EC'].mean().reset_index()\n",
        "    top_nodes_ec_df = pd.merge(count_ec, mean_ec, on='nodes')\n",
        "\n",
        "    return top_nodes_bc_df, top_nodes_dc_df, top_nodes_ec_df\n"
      ],
      "metadata": {
        "id": "q_NLERCNOB3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCULANO OS HUBS PARA OS GRAFOS 5%\n",
        "\n",
        "hubs_g_aut = get_top_nodes(grafos_aut)\n",
        "hubs_g_control = get_top_nodes(grafos_controll)\n"
      ],
      "metadata": {
        "id": "n1MOaPWUOIby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SEPARANDO OS HUBS POR METRICAS\n",
        "\n",
        "hubs_g_control_bc = hubs_g_control[0].head(40)\n",
        "hubs_g_control_dc = hubs_g_control[1].head(40)\n",
        "hubs_g_control_ec = hubs_g_control[2].head(40)\n",
        "\n",
        "hubs_g_aut_bc = hubs_g_aut[0].head(40)\n",
        "hubs_g_aut_dc = hubs_g_aut[1].head(40)\n",
        "hubs_g_aut_ec = hubs_g_aut[2].head(40)"
      ],
      "metadata": {
        "id": "_wC0ym9NOK2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ASSOCIANDO OS HUBS COM OS NOMES DAS REGIOES\n",
        "\n",
        "def associate_names(df, nomes):\n",
        "    node_names = []\n",
        "    for node in df['nodes']:\n",
        "        node = int(node)\n",
        "        if node <= len(nomes):\n",
        "            node_names.append(nomes[node - 1])\n",
        "        else:\n",
        "            node_names.append(None)\n",
        "    df['regiao'] = node_names\n",
        "    return df\n",
        "\n",
        "hubs_c_bc_aut_nomes = associate_names(hubs_g_aut_bc, nomes)\n",
        "hubs_c_dc_aut_nomes = associate_names(hubs_g_aut_dc, nomes)\n",
        "hubs_c_ec_aut_nomes = associate_names(hubs_g_aut_ec, nomes)\n",
        "\n",
        "hubs_c_bc_control_nomes = associate_names(hubs_g_control_bc, nomes)\n",
        "hubs_c_dc_control_nomes = associate_names(hubs_g_control_dc, nomes)\n",
        "hubs_c_ec_control_nomes = associate_names(hubs_g_control_ec, nomes)"
      ],
      "metadata": {
        "id": "Zcp7-5yXOOZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PARA GERAR OS HYPERGRAFOS USAMOS A MESMA FUNCAO ANTERIOR"
      ],
      "metadata": {
        "id": "i9RFI2e_Oapa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tri_control = HG_control(grafos_controll)\n",
        "tri_aut = HG_aut(grafos_aut)\n",
        "\n"
      ],
      "metadata": {
        "id": "wMSmmslEOeg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ESSAS TRES FUNCOES ACIMA FUNCIONAM JUNTAS  \n",
        "\n",
        "def construct_dual_graph(cliques): # AQUI CONSTRUIMOS OS GRAFOS DUAIS PARA CADA HYPERGRAFO A PARTIR DOS CLIQUES \n",
        "    dual_graph = nx.Graph()\n",
        "    node_hyperedge_map = {}\n",
        "    for hyperedge, clique in enumerate(cliques):\n",
        "        for node in clique:\n",
        "            if node not in node_hyperedge_map:\n",
        "                node_hyperedge_map[node] = set()\n",
        "            node_hyperedge_map[node].add(hyperedge)\n",
        "\n",
        "    for hyperedges in node_hyperedge_map.values():\n",
        "        for hyperedge1 in hyperedges:\n",
        "            for hyperedge2 in hyperedges:\n",
        "                if hyperedge1 != hyperedge2:\n",
        "                    dual_graph.add_edge(hyperedge1, hyperedge2)\n",
        "\n",
        "    return dual_graph\n",
        "\n",
        "\n",
        "def calculate_centralities(graph): # AQUI CALCULAMOS AS CENTRALIDADES DE CADA GRAFO DUAL\n",
        "    bc = nx.betweenness_centrality(graph)\n",
        "    dc = nx.degree_centrality(graph)\n",
        "    ec = nx.eigenvector_centrality(graph, max_iter=1000)\n",
        "\n",
        "    df = pd.DataFrame({'BC': bc, 'DC': dc, 'EC': ec})\n",
        "    df.index.name = 'Node'\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def calculate_dual_centralities(cliques): # AQUI CALCULAMOS OS HUBS DOS CLIQUES\n",
        "    dual_graph = construct_dual_graph(cliques)\n",
        "\n",
        "    centralities_df = calculate_centralities(dual_graph)\n",
        "\n",
        "    result_df = pd.DataFrame({'Clique': cliques})\n",
        "    result_df = result_df.join(centralities_df)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6-v6hgUkOpu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCULANDO OS HUBS DOS HYPERGRAFOS\n",
        "\n",
        "\n",
        "hubs_aut_hp = []\n",
        "for cliques in tri_aut:\n",
        "    c = calculate_dual_centralities(cliques)\n",
        "    hubs_aut_hp.append(c)\n",
        "\n",
        "hubs_control_hp = []\n",
        "for cliques in tri_control:\n",
        "    c = calculate_dual_centralities(cliques)\n",
        "    hubs_control_hp.append(c)\n"
      ],
      "metadata": {
        "id": "1j0Uw09oOt02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SEPARANDO OS HUBS POR METRICAS\n",
        "\n",
        "def split_dataframe(df, metric):\n",
        "    # Resetar o índice para tornar a coluna 'Clique' em uma coluna regular\n",
        "    df = df.reset_index()\n",
        "\n",
        "    # Ordenar o DataFrame pela métrica desejada em ordem decrescente\n",
        "    sorted_df = df.sort_values(by=metric, ascending=False)\n",
        "\n",
        "    # Criar o DataFrame separado contendo as colunas 'Clique' e a métrica\n",
        "    result_df = pd.DataFrame({'Clique': sorted_df['Clique'], metric: sorted_df[metric]})\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "hubs_aut_hp_split = []\n",
        "for df in hubs_aut_hp:\n",
        "    df_bc_aut = split_dataframe(df, 'BC')\n",
        "    df_dc_aut = split_dataframe(df, 'DC')\n",
        "    df_ec_aut = split_dataframe(df, 'EC')\n",
        "\n",
        "    # Adicionar os DataFrames separados à lista hubs_aut_hp_split\n",
        "    hubs_aut_hp_split.append(df_bc_aut)\n",
        "    hubs_aut_hp_split.append(df_dc_aut)\n",
        "    hubs_aut_hp_split.append(df_ec_aut)\n",
        "\n",
        "\n",
        "hubs_control_hp_split = []\n",
        "for df in hubs_control_hp:\n",
        "    df_bc_control = split_dataframe(df, 'BC')\n",
        "    df_dc_control = split_dataframe(df, 'DC')\n",
        "    df_ec_control = split_dataframe(df, 'EC')\n",
        "\n",
        "    # Adicionar os DataFrames separados à lista hubs_aut_hp_split\n",
        "    hubs_control_hp_split.append(df_bc_control)\n",
        "    hubs_control_hp_split.append(df_dc_control)\n",
        "    hubs_control_hp_split.append(df_ec_control)"
      ],
      "metadata": {
        "id": "tfm5s9wQO0yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ASSOCIANDO OS CLIQUES A REGIOES\n",
        "\n",
        "\n",
        "def associar_cliques_regioes(df, lista_nomes, metrica):\n",
        "    cliques_regioes = []\n",
        "    for clique in df['Clique']:\n",
        "        regioes_clique = [lista_nomes[indice-1] for indice in clique]\n",
        "        cliques_regioes.append(regioes_clique)\n",
        "\n",
        "    df_resultado = pd.DataFrame({'Clique': df['Clique'], metrica: df[metrica], 'Regioes': cliques_regioes})\n",
        "    return df_resultado\n",
        "\n",
        "df_bc_aut_nomes = associar_cliques_regioes(df_bc_aut, nomes, 'BC')\n",
        "df_dc_aut_nomes = associar_cliques_regioes(df_dc_aut, nomes, 'DC')\n",
        "df_ec_aut_nomes = associar_cliques_regioes(df_ec_aut, nomes, 'EC')\n",
        "\n",
        "df_bc_control_nomes = associar_cliques_regioes(df_bc_control, nomes, 'BC')\n",
        "df_dc_control_nomes = associar_cliques_regioes(df_dc_control, nomes, 'DC')\n",
        "df_ec_control_nomes = associar_cliques_regioes(df_ec_control, nomes, 'EC')"
      ],
      "metadata": {
        "id": "hFXFkLEiO3BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$######\n",
        "# AQUI VAMOS SEPARAR OS HUBS POR DATAFRAMES"
      ],
      "metadata": {
        "id": "F2JjDcFhQK2n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COLOCANDO _ ENTRE OS NOMES DAS REGIOES PARA EVITAR OVERCLICK DAS PALAVRAS RIGHT, LEFT, ...\n",
        "\n",
        "def adicionar_underscore(nomes):\n",
        "    nomes_com_underscore = []\n",
        "    \n",
        "    for nome in nomes:\n",
        "        nome_com_underscore = nome.replace(\" \", \"_\")\n",
        "        nomes_com_underscore.append(nome_com_underscore)\n",
        "    \n",
        "    return nomes_com_underscore\n",
        "\n",
        "nomes = adicionar_underscore(nomes)\n",
        "\n",
        "# AQUI IMPORTEI OS HUBS QUE FORAM COMPUTADOS ANTERIORMENTE\n",
        "\n",
        "base_path = '/content/drive/MyDrive/metricas/graficos filtrados/dfs'\n",
        "folders = ['hubs g disp', 'hubs hp disp', 'hubs g 5%', 'hubs hp 5%']\n",
        "hubs_g_disp_list = []\n",
        "hubs_hp_disp_list = []\n",
        "hubs_g_5pct_list = []\n",
        "hubs_hp_5pct_list = []\n",
        "\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            if folder == 'hubs g disp':\n",
        "                hubs_g_disp_list.append(df)\n",
        "            elif folder == 'hubs hp disp':\n",
        "              \n",
        "              hubs_hp_disp_list.append(df)\n",
        "            elif folder == 'hubs g 5%':\n",
        "                hubs_g_5pct_list.append(df)\n",
        "            elif folder == 'hubs hp 5%':\n",
        "                hubs_hp_5pct_list.append(df)\n"
      ],
      "metadata": {
        "id": "LNCkJbSLQfOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HUBS DOS GRAFOS DISPARITY FILTER\n",
        "\n",
        "hubs_g_disp_aut_bc = hubs_g_disp_list[3]\n",
        "hubs_g_disp_aut_dc = hubs_g_disp_list[4]\n",
        "hubs_g_disp_aut_ec = hubs_g_disp_list[5]\n",
        "hubs_g_disp_control_bc = hubs_g_disp_list[0]\n",
        "hubs_g_disp_control_dc = hubs_g_disp_list[1]\n",
        "hubs_g_disp_control_ec = hubs_g_disp_list[2]"
      ],
      "metadata": {
        "id": "BCHCaILBQoem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HUBS DOS HYPERGRAFOS DISPARITY FILTER\n",
        "\n",
        "hubs_hp_disp_aut_bc = hubs_hp_disp_list[0]\n",
        "hubs_hp_disp_aut_dc = hubs_hp_disp_list[1]\n",
        "hubs_hp_disp_aut_ec = hubs_hp_disp_list[2]\n",
        "\n",
        "hubs_hp_disp_control_bc = hubs_hp_disp_list[3]\n",
        "hubs_hp_disp_control_dc = hubs_hp_disp_list[4]\n",
        "hubs_hp_disp_control_ec = hubs_hp_disp_list[4]"
      ],
      "metadata": {
        "id": "nqOSJnF5Qz8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HUBS DOS GRAFOS 5% \n",
        "\n",
        "hubs_g_5pct_aut_bc = hubs_g_5pct_list[0]\n",
        "hubs_g_5pct_aut_dc = hubs_g_5pct_list[1]\n",
        "hubs_g_5pct_aut_ec = hubs_g_5pct_list[2]\n",
        "\n",
        "hubs_g_5pct_control_bc = hubs_g_5pct_list[3]\n",
        "hubs_g_5pct_control_dc = hubs_g_5pct_list[4]\n",
        "hubs_g_5pct_control_ec = hubs_g_5pct_list[4]"
      ],
      "metadata": {
        "id": "bXyOLhQgQ2uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HUBS DOS HYPERGRAFOS 5%\n",
        "\n",
        "hubs_hp_5pct_bc_aut = hubs_hp_5pct_list[0]\n",
        "hubs_hp_5pct_dc_aut = hubs_hp_5pct_list[1]\n",
        "hubs_hp_5pct_ec_aut = hubs_hp_5pct_list[2]\n",
        "\n",
        "hubs_hp_5pct_bc_control = hubs_hp_5pct_list[3]\n",
        "hubs_hp_5pct_dc_control = hubs_hp_5pct_list[4]\n",
        "hubs_hp_5pct_ec_control = hubs_hp_5pct_list[5]"
      ],
      "metadata": {
        "id": "Z4JUkacmQ4Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "def encontrar_nos_comuns_5(hubs_df, clique_df): # AQUI CALCULAMOS OS NODES COMUNS NOS HUBS CLIQUES QUE TAMBEM APARECEM NOS HUBS DOS GRAFOS \n",
        "    # Extrair os nós dos dataframes\n",
        "    nodes_hubs = hubs_df['nodes'].tolist()\n",
        "    nodes_clique = [ast.literal_eval(clique) for clique in clique_df['Clique']]\n",
        "\n",
        "    # Flattening para obter uma lista plana de nós\n",
        "    flattened_nodes_clique = [node for sublist in nodes_clique for node in sublist]\n",
        "\n",
        "    # Encontrar nós comuns\n",
        "    nos_comuns = list(set(nodes_hubs) & set(flattened_nodes_clique))\n",
        "\n",
        "    # Criar um dicionário para armazenar as informações dos nós comuns\n",
        "    nos_dict = {'nodes': nos_comuns, 'freq': [], 'regiao': []}\n",
        "\n",
        "    # Preencher o dicionário com a frequência e região associada a cada nó\n",
        "    for node in nos_comuns:\n",
        "        freq = flattened_nodes_clique.count(node)\n",
        "        regiao = hubs_df.loc[hubs_df['nodes'] == node, 'regiao'].values[0]\n",
        "        nos_dict['freq'].append(freq)\n",
        "        nos_dict['regiao'].append(regiao)\n",
        "\n",
        "    # Preencher com nós que não aparecem em comum com frequência 0\n",
        "    for node in range(1, 265):\n",
        "        if node not in nos_dict['nodes']:\n",
        "            nos_dict['nodes'].append(node)\n",
        "            nos_dict['freq'].append(0)\n",
        "            nos_dict['regiao'].append('')\n",
        "\n",
        "    # Criar o dataframe com os nós comuns, frequência e região\n",
        "    df = pd.DataFrame(nos_dict)\n",
        "\n",
        "    # Ordenar o dataframe em ordem decrescente de frequência\n",
        "    df = df.sort_values('freq', ascending=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# NODES DOS HUBS DOS HYPERGRAFOS QUE SAO HUBS DOS GRAFOS TAMBEM PARA 5%\n",
        "\n",
        "freq_hubs_aut_bc_5 = encontrar_nos_comuns_5(hubs_g_5pct_aut_bc, hubs_hp_5pct_bc_aut).head(15)\n",
        "freq_hubs_aut_dc_5 = encontrar_nos_comuns_5(hubs_g_5pct_aut_dc, hubs_hp_5pct_dc_aut).head(15)\n",
        "freq_hubs_aut_ec_5 = encontrar_nos_comuns_5(hubs_g_5pct_aut_ec, hubs_hp_5pct_ec_aut).head(15)\n",
        "\n",
        "freq_hubs_control_bc_5 = encontrar_nos_comuns_5(hubs_g_5pct_control_bc, hubs_hp_5pct_bc_control).head(15)\n",
        "freq_hubs_control_dc_5 = encontrar_nos_comuns_5(hubs_g_5pct_control_dc, hubs_hp_5pct_dc_control).head(15)\n",
        "freq_hubs_control_ec_5 = encontrar_nos_comuns_5(hubs_g_5pct_control_ec, hubs_hp_5pct_ec_control).head(15)"
      ],
      "metadata": {
        "id": "l3SiR4FbQ7Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def encontrar_nos_comuns(hubs_df, clique_df): # FAZEMOS O MESMO AQUI, PRECISOU ADAPTAR A FUNCAO POIS OS CLIQUES ESTAO ENTRE ' '\n",
        "    # Extrair os nós dos dataframes\n",
        "    nodes_hubs = hubs_df['nodes'].tolist()\n",
        "    nodes_clique = [clique.strip(\"[]'\").split(\"', '\") for clique in clique_df['Clique']]\n",
        "\n",
        "    # Criar um dicionário para armazenar as informações dos nós comuns\n",
        "    nos_dict = {'nodes': [], 'freq': [], 'regiao': []}\n",
        "\n",
        "    # Percorrer os nós do dataframe hubs_df\n",
        "    for node in nodes_hubs:\n",
        "        freq = sum([str(node) in clique for clique in nodes_clique])\n",
        "        regiao = hubs_df.loc[hubs_df['nodes'] == node, 'regiao'].values[0]\n",
        "        nos_dict['nodes'].append(node)\n",
        "        nos_dict['freq'].append(freq)\n",
        "        nos_dict['regiao'].append(regiao)\n",
        "\n",
        "    # Preencher com nós que não aparecem em comum com frequência 0\n",
        "    for node in range(1, 265):\n",
        "        if node not in nos_dict['nodes']:\n",
        "            nos_dict['nodes'].append(node)\n",
        "            nos_dict['freq'].append(0)\n",
        "            nos_dict['regiao'].append('')\n",
        "\n",
        "    # Criar o dataframe com os nós comuns, frequência e região\n",
        "    df = pd.DataFrame(nos_dict)\n",
        "\n",
        "    # Ordenar o dataframe em ordem decrescente de frequência\n",
        "    df = df.sort_values('freq', ascending=False)\n",
        "\n",
        "    return df\n",
        "# NODES DOS HUBS DOS HYPERGRAFOS QUE SAO HUBS DOS GRAFOS TAMBEM PARA DISPARITY\n",
        "\n",
        "freq_hubs_aut_disp_bc = encontrar_nos_comuns(hubs_g_disp_aut_bc, hubs_hp_disp_aut_bc).head(15)\n",
        "freq_hubs_aut_disp_dc = encontrar_nos_comuns(hubs_g_disp_aut_dc, hubs_hp_disp_aut_dc).head(15)\n",
        "freq_hubs_aut_disp_ec = encontrar_nos_comuns(hubs_g_disp_aut_ec, hubs_hp_disp_aut_ec).head(15)\n",
        "\n",
        "freq_hubs_control_disp_bc = encontrar_nos_comuns(hubs_g_disp_control_bc, hubs_hp_disp_control_bc).head(15)\n",
        "freq_hubs_control_disp_dc = encontrar_nos_comuns(hubs_g_disp_control_dc, hubs_hp_disp_control_dc).head(15)\n",
        "freq_hubs_control_disp_ec = encontrar_nos_comuns(hubs_g_disp_control_ec, hubs_hp_disp_control_ec).head(15)"
      ],
      "metadata": {
        "id": "x90zN2ePRFD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AGORA VOU IMPRIMIR OS NODES COMUNS\n",
        "\n",
        "\n",
        "# Métrica BC\n",
        "cliques_bc = hubs_hp_disp_aut_bc['Clique'].apply(eval)  # Converter as strings para listas\n",
        "cliques_bc = cliques_bc.apply(lambda x: [str(node) for node in x])  # Converter os nós para strings\n",
        "common_nodes_bc = set(cliques_bc.sum()) & set(hubs_g_disp_aut_bc['nodes'].astype(str))\n",
        "common_nodes_bc_aut_d = set(nomes[int(node)-1] for node in common_nodes_bc if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós comuns (BC):\", common_nodes_bc_aut_d)\n",
        "\n",
        "# Métrica DC\n",
        "cliques_dc = hubs_hp_disp_aut_dc['Clique'].apply(eval)  # Converter as strings para listas\n",
        "cliques_dc = cliques_dc.apply(lambda x: [str(node) for node in x])  # Converter os nós para strings\n",
        "common_nodes_dc = set(cliques_dc.sum()) & set(hubs_g_disp_aut_dc['nodes'].astype(str))\n",
        "common_nodes_dc_aut_d = set(nomes[int(node)-1] for node in common_nodes_dc if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós comuns (DC):\", common_nodes_dc_aut_d)\n",
        "\n",
        "# Métrica EC\n",
        "cliques_ec = hubs_hp_disp_aut_ec['Clique'].apply(eval)  # Converter as strings para listas\n",
        "cliques_ec = cliques_ec.apply(lambda x: [str(node) for node in x])  # Converter os nós para strings\n",
        "common_nodes_ec = set(cliques_ec.sum()) & set(hubs_g_disp_aut_ec['nodes'].astype(str))\n",
        "common_nodes_ec_aut_d = set(nomes[int(node)-1] for node in common_nodes_ec if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós comuns (EC):\", common_nodes_ec_aut_d)\n"
      ],
      "metadata": {
        "id": "YYMiTzmYRNLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Métrica BC\n",
        "cliques_bc = hubs_hp_disp_control_bc['Clique'].apply(eval)  # Converter as strings para listas\n",
        "cliques_bc = cliques_bc.apply(lambda x: [str(node) for node in x])  # Converter os nós para strings\n",
        "common_nodes_bc = set(cliques_bc.sum()) & set(hubs_g_disp_control_bc['nodes'].astype(str))\n",
        "common_nodes_bc_control_d = set(nomes[int(node)-1] for node in common_nodes_bc if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós comuns (BC):\", common_nodes_bc_control_d)\n",
        "\n",
        "# Métrica DC\n",
        "cliques_dc = hubs_hp_disp_control_dc['Clique'].apply(eval)  # Converter as strings para listas\n",
        "cliques_dc = cliques_dc.apply(lambda x: [str(node) for node in x])  # Converter os nós para strings\n",
        "common_nodes_dc = set(cliques_dc.sum()) & set(hubs_g_disp_control_dc['nodes'].astype(str))\n",
        "common_nodes_dc_control_d = set(nomes[int(node)-1] for node in common_nodes_dc if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós comuns (DC):\", common_nodes_dc_control_d)\n",
        "\n",
        "# Métrica EC\n",
        "cliques_ec = hubs_hp_disp_control_ec['Clique'].apply(eval)  # Converter as strings para listas\n",
        "cliques_ec = cliques_ec.apply(lambda x: [str(node) for node in x])  # Converter os nós para strings\n",
        "common_nodes_ec = set(cliques_ec.sum()) & set(hubs_g_disp_control_ec['nodes'].astype(str))\n",
        "common_nodes_ec_control_d = set(nomes[int(node)-1] for node in common_nodes_ec if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós comuns (EC):\", common_nodes_ec_control_d)"
      ],
      "metadata": {
        "id": "GEsjkHuDRaX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Nós exclusivos dos hypergrafos (EC)\n",
        "exclusive_nodes_hypergraph_ec = set(hubs_hp_disp_aut_ec['Clique'].sum()) - set(hubs_g_disp_aut_ec['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_ec_aut_d = set(nomes[int(node)] for node in exclusive_nodes_hypergraph_ec if node.isdigit())\n",
        "print(\"Nós exclusivos dos hypergrafos (EC):\", exclusive_nodes_hypergraph_ec_aut_d)\n",
        "\n",
        "# Nós exclusivos dos hypergrafos (BC)\n",
        "exclusive_nodes_hypergraph_bc = set(hubs_hp_disp_aut_bc['Clique'].sum()) - set(hubs_g_disp_aut_bc['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_bc_aut_d = set(nomes[int(node)] for node in exclusive_nodes_hypergraph_bc if node.isdigit())\n",
        "print(\"Nós exclusivos dos hypergrafos (BC):\", exclusive_nodes_hypergraph_bc_aut_d)\n",
        "\n",
        "# Nós exclusivos dos hypergrafos (DC)\n",
        "exclusive_nodes_hypergraph_dc = set(hubs_hp_disp_aut_dc['Clique'].sum()) - set(hubs_g_disp_aut_dc['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_dc_aut_d = set(nomes[int(node)] for node in exclusive_nodes_hypergraph_dc if node.isdigit())\n",
        "print(\"Nós exclusivos dos hypergrafos (DC):\", exclusive_nodes_hypergraph_dc_aut_d)\n"
      ],
      "metadata": {
        "id": "8eTrSeBaRvNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Nós exclusivos dos hypergrafos (EC)\n",
        "exclusive_nodes_hypergraph_ec = set(hubs_hp_disp_control_ec['Clique'].sum()) - set(hubs_g_disp_control_ec['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_ec_control_d = set(nomes[int(node)] for node in exclusive_nodes_hypergraph_ec if node.isdigit())\n",
        "print(\"Nós exclusivos dos hypergrafos (EC):\", exclusive_nodes_hypergraph_ec_control_d)\n",
        "\n",
        "# Nós exclusivos dos hypergrafos (BC)\n",
        "exclusive_nodes_hypergraph_bc = set(hubs_hp_disp_control_bc['Clique'].sum()) - set(hubs_g_disp_control_bc['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_bc_control_d = set(nomes[int(node)] for node in exclusive_nodes_hypergraph_bc if node.isdigit())\n",
        "print(\"Nós exclusivos dos hypergrafos (BC):\", exclusive_nodes_hypergraph_bc_control_d)\n",
        "\n",
        "# Nós exclusivos dos hypergrafos (DC)\n",
        "exclusive_nodes_hypergraph_dc = set(hubs_hp_disp_control_dc['Clique'].sum()) - set(hubs_g_disp_control_dc['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_dc_control_d = set(nomes[int(node)] for node in exclusive_nodes_hypergraph_dc if node.isdigit())\n",
        "print(\"Nós exclusivos dos hypergrafos (DC):\", exclusive_nodes_hypergraph_dc_control_d)\n"
      ],
      "metadata": {
        "id": "zJ8cKlFpRamo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Nós exclusivos dos hypergrafos (EC)\n",
        "exclusive_nodes_hypergraph_ec = set(hubs_hp_5pct_ec_aut['Clique'].sum()) - set(hubs_g_5pct_aut_ec['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_ec_aut_5 = set(nomes[int(node)-1] for node in exclusive_nodes_hypergraph_ec if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós exclusivos dos hypergrafos (EC):\", exclusive_nodes_hypergraph_ec_aut_5)\n",
        "\n",
        "# Nós exclusivos dos hypergrafos (BC)\n",
        "exclusive_nodes_hypergraph_bc = set(hubs_hp_5pct_bc_aut['Clique'].sum()) - set(hubs_g_5pct_aut_bc['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_bc_aut_5 = set(nomes[int(node)-1] for node in exclusive_nodes_hypergraph_bc if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós exclusivos dos hypergrafos (BC):\", exclusive_nodes_hypergraph_bc_aut_5)\n",
        "\n",
        "# Nós exclusivos dos hypergrafos (DC)\n",
        "exclusive_nodes_hypergraph_dc = set(hubs_hp_5pct_dc_aut['Clique'].sum()) - set(hubs_g_5pct_aut_dc['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_dc_aut_5 = set(nomes[int(node)-1] for node in exclusive_nodes_hypergraph_dc if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós exclusivos dos hypergrafos (DC):\", exclusive_nodes_hypergraph_dc_aut_5)\n"
      ],
      "metadata": {
        "id": "ww06bES2Raov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nós exclusivos dos hypergrafos (EC)\n",
        "exclusive_nodes_hypergraph_ec = set(hubs_hp_5pct_ec_control['Clique'].sum()) - set(hubs_g_5pct_control_ec['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_ec_control_5 = set(nomes[int(node)-1] for node in exclusive_nodes_hypergraph_ec if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós exclusivos dos hypergrafos (EC):\", exclusive_nodes_hypergraph_ec_control_5)\n",
        "\n",
        "# Nós exclusivos dos hypergrafos (BC)\n",
        "exclusive_nodes_hypergraph_bc = set(hubs_hp_5pct_bc_control['Clique'].sum()) - set(hubs_g_5pct_control_bc['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_bc_control_5 = set(nomes[int(node)-1] for node in exclusive_nodes_hypergraph_bc if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós exclusivos dos hypergrafos (BC):\", exclusive_nodes_hypergraph_bc_control_5)\n",
        "\n",
        "# Nós exclusivos dos hypergrafos (DC)\n",
        "exclusive_nodes_hypergraph_dc = set(hubs_hp_5pct_dc_control['Clique'].sum()) - set(hubs_g_5pct_control_dc['nodes'].astype(str))\n",
        "exclusive_nodes_hypergraph_dc_control_5 = set(nomes[int(node)-1] for node in exclusive_nodes_hypergraph_dc if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós exclusivos dos hypergrafos (DC):\", exclusive_nodes_hypergraph_dc_control_5)\n"
      ],
      "metadata": {
        "id": "6ZSbDralRaq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Métrica BC\n",
        "cliques_bc = hubs_hp_5pct_bc_aut['Clique'].apply(eval)  # Converter as strings para listas\n",
        "cliques_bc = cliques_bc.apply(lambda x: [str(node) for node in x])  # Converter os nós para strings\n",
        "common_nodes_bc = set(cliques_bc.sum()) & set(hubs_g_5pct_aut_bc['nodes'].astype(str))\n",
        "common_nodes_bc_aut_5  = set(nomes[int(node)-1] for node in common_nodes_bc if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós comuns (BC):\", common_nodes_bc_aut_5 )\n",
        "\n",
        "# Métrica DC\n",
        "cliques_dc = hubs_hp_5pct_dc_aut['Clique'].apply(eval)  # Converter as strings para listas\n",
        "cliques_dc = cliques_dc.apply(lambda x: [str(node) for node in x])  # Converter os nós para strings\n",
        "common_nodes_dc = set(cliques_dc.sum()) & set(hubs_g_5pct_aut_dc['nodes'].astype(str))\n",
        "common_nodes_dc_aut_5  = set(nomes[int(node)-1] for node in common_nodes_dc if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós comuns (DC):\", common_nodes_dc_aut_5)\n",
        "\n",
        "# Métrica EC\n",
        "cliques_ec = hubs_hp_5pct_ec_aut['Clique'].apply(eval)  # Converter as strings para listas\n",
        "cliques_ec = cliques_ec.apply(lambda x: [str(node) for node in x])  # Converter os nós para strings\n",
        "common_nodes_ec = set(cliques_ec.sum()) & set(hubs_g_5pct_aut_ec['nodes'].astype(str))\n",
        "common_nodes_ec_aut_5  = set(nomes[int(node)-1] for node in common_nodes_ec if node.isdigit() and int(node)-1 < len(nomes))\n",
        "print(\"Nós comuns (EC):\", common_nodes_ec_aut_5)"
      ],
      "metadata": {
        "id": "o_gbmMv0R9FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AQUI GEREI OS WORDCLOUDS\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def plot_wordclouds(common_nodes_bc, common_nodes_dc, common_nodes_ec):\n",
        "    # Criar WordCloud para a Métrica BC\n",
        "    wordcloud_bc = WordCloud(background_color='white', width=400, height=300).generate(' '.join(common_nodes_bc))\n",
        "\n",
        "    # Criar WordCloud para a Métrica DC\n",
        "    wordcloud_dc = WordCloud(background_color='white', width=400, height=300).generate(' '.join(common_nodes_dc))\n",
        "\n",
        "    # Criar WordCloud para a Métrica EC\n",
        "    wordcloud_ec = WordCloud(background_color='white', width=400, height=300).generate(' '.join(common_nodes_ec))\n",
        "\n",
        "    # Configurar a figura com três subplots lado a lado\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "    # Plotar WordCloud para a Métrica BC\n",
        "    axes[0].imshow(wordcloud_bc, interpolation='bilinear')\n",
        "    axes[0].set_title('Hubs comuns G x HG TD BC')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Plotar WordCloud para a Métrica DC\n",
        "    axes[1].imshow(wordcloud_dc, interpolation='bilinear')\n",
        "    axes[1].set_title('Hubs comuns G x HG TD DC')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Plotar WordCloud para a Métrica EC\n",
        "    axes[2].imshow(wordcloud_ec, interpolation='bilinear')\n",
        "    axes[2].set_title('Hubs comuns G x HG TD EC')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Ajustar espaçamento entre os subplots\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Exibir a figura\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oivIQ2ZkRas5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "olmKu83sRavA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2sZXyWORaxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmfhU0z6RazJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aW4DrldWRa1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vI9lVPBMRa3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nyAnL33ARa7H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
